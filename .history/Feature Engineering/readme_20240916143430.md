# Feature Engineering

Feature engineering is a crucial part of the data preprocessing step in machine learning. It involves transforming raw data into features that better represent the underlying patterns to improve model accuracy.

### 1. Understanding Features

**Feature**: A variable or column in your dataset that helps in predicting the target. For instance, in a house price prediction dataset, features could be the number of rooms, area in square feet, etc.

**Target**: The output or label you are trying to predict. In the house price example, the target is the price

### Key Feature Engineering Techniques

1. Handling Missing Data
   Missing data can distort model predictions. Common techniques to handle missing data:

Imputation: Replacing missing values with the mean, median, or mode.
Dropping: Removing rows or columns with too many missing values.

2. Encoding Categorical Variables
   Machine learning models require numeric input, so categorical features must be converted into numbers.

Label Encoding: Assigns a unique number to each category.
One-Hot Encoding: Creates binary columns for each category.
Ordinal Encoding: Encodes categories with meaningful order (e.g., small, medium, large).

3. Feature Scaling
   Numerical features often need scaling to ensure all features contribute equally to the model.

Standardization: Rescales features to have a mean of 0 and standard deviation of 1.
Normalization: Rescales features to a range between 0 and 1.

4. Binning
   Transforms continuous features into categorical features by dividing them into bins or intervals.

Example: Age can be divided into bins like 0–18, 19–35, 36–60, etc.

5. Feature Transformation
   Some features need transformation for the model to learn better.

Log Transform: Useful for skewed data to make it more normally distributed.
Polynomial Features: Adding squared or interaction terms for non-linear relationships.

6. Feature Creation
   You can create new features that may provide more insight.

Interaction Features: Multiplying or dividing existing features (e.g., income/age).
Date-Time Features: Extracting features like year, month, day, hour, or weekday from datetime columns.

7. Dimensionality Reduction
   Too many features can lead to overfitting or slow computation. Reducing the number of features while retaining most information can help.

Principal Component Analysis (PCA): A technique that reduces feature space by finding a few new variables (principal components) that account for the most variance in the data.
Feature Selection: Select important features based on correlation or model importance.
